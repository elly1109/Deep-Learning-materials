{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVO3rqIH4UMj"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Neural Style Transfer (NST), is the technique in which the style of a piece of artwork is transferred onto a picture. It basically merges two images, namely, a \"content\" image (C) and a \"style\" image (S), to create a \"generated\" image (G). The generated image G combines the \"content\" of the image C with the \"style\" of image S.\n",
    "\n",
    "In the training process, the neural network is not trained  to do anything instead it make use of of backpropagation to minimize two defined loss values the content image loss $J_c$ and the the style loss $J_s$ as defined below. To put it in another way, while training neural networks we update our weights and biases, but in style transfer, we keep the weights and biases constant, and instead, update our image.\n",
    "\n",
    "The principle is simple: we define two losses, one for the content image ($J_c$) and one for the style ($J_s$). $J_c$ measures how different the content is between two images, while $J_s$ measures how different the style is between two images. Then, we take a third image, the input, (e.g. a with noise), and we transform it in order to both minimize its content-loss with the content-image and its style-distance with the style-image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOhTrHKR4dm0"
   },
   "source": [
    "## 2. Define content and style loss\n",
    "\n",
    "### 2.1. Content loss\n",
    "\n",
    "The content loss is the *euclidean distance* between the intermediate representations of the **pastiche** image $P_{ij}$(generated image) and the content image $C_{ij}$ defined by:\n",
    "\n",
    "\n",
    "\\begin{align}J_c =  \\sum_l \\sum_{i,j} (\\alpha C_{i,j}^l - \\alpha P_{i,j}^l)^2\\end{align}\n",
    "\n",
    " that can be computed using a criterion `nn.MSELoss()`.\n",
    " \n",
    "From the above equation is clear that, you first make a list of layers at which you want to compute content loss and then  pass the content and pastiches images through the network until a particular layer in the list. Aftet this, you take the output of that layer, square the difference between each corresponding value in the output, and sum them all up. \n",
    "\n",
    "The process is repeated for every layer in the list, and sum those up. One thing to note, though: we multiply each of the representations by some value alpha (called the content weight) before finding their differences and squaring it, whereas the original equation calls for the value to be multiplied after squaring it. In practice, the former  work much better than the latter, as it produces appealing stylizations much more quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L4GFmVW4l5r"
   },
   "source": [
    "### 2.2 Style cost\n",
    "For the style loss, we need first to define a module that compute the\n",
    "style matrix, also called a \"Gram matrix\" $G$ given the feature maps $P$ of the\n",
    "neural network fed by $C$, at layer $l$. In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\\dots ,v_{n})$ is the matrix of dot products, whose entries are $${\\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$$ In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product.\n",
    "\n",
    "The Gram matrix contains non-localized information about the image, such as texture, shapes, and weights - style. The pytorch implementation of gram matrix is shown below.\n",
    "\n",
    "```python\n",
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        return G.div(a * b * c * d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYlVT58P4q1_"
   },
   "source": [
    "Now that we have defined the Gram matrix as having information about style, the style loss is then the euclidean distance between the Gram matrices of the intermediate representations of the pastiche $P$ and style image $S$.\n",
    "\n",
    "\\begin{align}J_s =  \\sum_l \\sum_{i,j} (\\beta G_{i,j}^{s,l} - \\beta G_{i,j}^{p,l})^2\\end{align}\n",
    "\n",
    "Similar to the content loss computation, we find the Euclidean distances between each corresponding pair of values in the Gram matrices computed at each layer in a predefined list of layers, multiplied by some value $\\beta$ (known as the style weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCmdlU5p4wrY"
   },
   "source": [
    "### 2.3 Total Loss\n",
    "\n",
    "Given the content loss ($J_c$) which contains information on how close the pastiche is in content to the content image and the style loss ($J_s$) — which contains information on how close the pastiche is in style to the style image. We can now add them together to get the total loss ($J$). \n",
    "\n",
    "$$\n",
    "J = J_c + J_s\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We then backpropagate through the network to reduce this loss by getting a gradient on the pastiche image and iteratively changing it to make it look more and more like a stylized content image.\n",
    "\n",
    "Then, the style loss module is implemented exactly the same way like the content loss module, but we have to add the gramMatrix as a parameter:\n",
    "\n",
    "```python\n",
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUFghMxz447r"
   },
   "source": [
    "## 3 Pytorch implementation\n",
    "\n",
    "### 3.1 Import all important line\n",
    "\n",
    "First install pytorch 0.4.0 for colaraboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2882,
     "status": "ok",
     "timestamp": 1525945562464,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "T-4dv9IKtrdu",
    "outputId": "5dfa0979-f58d-4479-b420-14c30504f813"
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eP1uFaWHV5i"
   },
   "source": [
    "Then load all  important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "wNMnhEdE5E4G"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import scipy\n",
    "from scipy.misc import imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lDMR9ByQM0o7"
   },
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PMAVMA0L5bIp"
   },
   "source": [
    "## 3.2 Define network\n",
    "\n",
    "We first create a class which  initialize  important variables such as \n",
    "- the style,  content, and pastiches images;\n",
    "- the layers at which we compute content and style loss, as well as the alpha and beta weights we multiply the representations by; \n",
    "- the pretrained network that we use to get the intermediate representations (we use VGG-19); \n",
    "- the style loss and the content  loss  and \n",
    "- the optimizer we use. \n",
    "\n",
    "We also want to make use of a GPU if we have on on our machine. Now for the network’s “training” regime. We pass the images through the network one layer at a time. We check to see if it is a layer at which we do a content or style loss computation. If it is, we compute the appropriate loss at that layer. Finally, we add the content and style losses together and call backward on that loss, and take an update step. \n",
    "\n",
    "To use the LBGFS optimizer, it is necessary to pass into the step function a closure function which “reevaluates the model and returns the loss”; we don’t need to do that with any other optimizer…go figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "em1gq_9-5gJJ"
   },
   "outputs": [],
   "source": [
    "class StyleCNN(object):\n",
    "  \n",
    "    def __init__(self,  style, content, pastiche):\n",
    "        super(StyleCNN, self).__init__()\n",
    "        self.device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.style = style.to(self.device)\n",
    "        self.content = content.to(self.device)\n",
    "        pastiche     = pastiche.to(self.device)\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1000\n",
    "        \n",
    "        self.model = models.vgg19(pretrained=True)\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.gram = GramMatrix()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])\n",
    "        self.model.to(self.device)\n",
    "        self.mse.to(self.device)\n",
    "        self.gram.to(self.device)\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "            pastiche = self.pastiche.clone()\n",
    "            pastiche.data.clamp_(0, 1)\n",
    "            content = self.content.clone()\n",
    "            style = self.style.clone()\n",
    "            \n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "            \n",
    "            i = 1\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            \n",
    "            for layer in list(self.model.features):\n",
    "                layer = not_inplace(layer)\n",
    "                \n",
    "                    \n",
    "                pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "                \n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = \"conv_\" + str(i)\n",
    "                    \n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.mse(pastiche * self.content_weight,\n",
    "                                                         content.detach() * self.content_weight)\n",
    "                    \n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                        style_loss += self.mse(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "                \n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "            \n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            return total_loss\n",
    "        self.optimizer.step(closure)\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9pNYAX-5mwL"
   },
   "source": [
    "## 3.3 Function to load and process image\n",
    "\n",
    "One more step before we can start transferring some style — we need to write up a couple of convenience functions to load and visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HzM9kPbK5wc0"
   },
   "outputs": [],
   "source": [
    "imsize = 128\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize((imsize, imsize)),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image\n",
    "  \n",
    "def save_image(input, path):\n",
    "    image = input.data.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    imsave(path, image)    \n",
    "    \n",
    "def imshow(tensor, title=None):\n",
    "    plt.ion()\n",
    "    image = tensor.clone()  \n",
    "    image = image.squeeze(0)      \n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eskXE6NW6_Df"
   },
   "source": [
    "## 3.4 Load content and style images\n",
    "\n",
    "Let write function to load image from computer. You only need this for colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cqkhrGmLBoBG"
   },
   "outputs": [],
   "source": [
    "def upload_files():\n",
    "  from google.colab import files\n",
    "  uploaded = files.upload()\n",
    "  for k, v in uploaded.items():\n",
    "    open(k, 'wb').write(v)\n",
    "  return list(uploaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 126,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 32575,
     "status": "ok",
     "timestamp": 1525945633695,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "79NNnpp9DOeA",
    "outputId": "30f1a106-7169-48b9-f11d-929b8ff2f552"
   },
   "outputs": [],
   "source": [
    "upload_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81sRhnF1KR9w"
   },
   "source": [
    "Load content and style images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 848,
     "status": "error",
     "timestamp": 1525945663593,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "FerZXrp87Dqb",
    "outputId": "0b304eb1-59ec-4857-8daa-9faa0950b404"
   },
   "outputs": [],
   "source": [
    "style_img = image_loader(\"../Data/bread.jpg\")\n",
    "content_img = image_loader(\"../Data/bondia.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1295,
     "status": "ok",
     "timestamp": 1525905868272,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "2jGyGJh1Egnb",
    "outputId": "6521fffd-cbec-48ac-b3f0-b68b40a5ff53"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "imshow(style_img, title='Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1812,
     "status": "ok",
     "timestamp": 1525905880318,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "qWkt4SNxE-si",
    "outputId": "76d9a308-fc7a-441d-b7ed-c92a2e4eebdb"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "imshow(content_img, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i84BRcoeKYvN"
   },
   "source": [
    "Define a function to train our models. We will train our model for 20 iteration and save image  as *pastiche_n.png* ater every  $n$  epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2d-3F3xjKpP4"
   },
   "outputs": [],
   "source": [
    "def main(style, content, pastiche,  num_epochs):\n",
    "    \n",
    "    style_cnn = StyleCNN(style, content, pastiche)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        pastiche = style_cnn.train()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "          \n",
    "          print(\"Iteration: %d\" % (i))\n",
    "          pastiche.data.clamp_(0, 1)\n",
    "          path = \"pastiche_{}.png\".format(i+1)\n",
    "          save_image(pastiche, path)\n",
    "            \n",
    "        if i == num_epochs - 1:\n",
    "          print(\"save image at {}\".format(i+1))\n",
    "          pastiche.data.clamp_(0, 1)\n",
    "          path = \"pastiche_{}.png\".format(i+1)\n",
    "          save_image(pastiche, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZtcKD_8LII3"
   },
   "source": [
    "### Train model\n",
    "Before we call our model let define initial pastiche image.To simplify the code, we take an image of the same dimensions like content  images. This image can be a white noise, or it can also be a copy of the content-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "V26BlqeELc7G"
   },
   "outputs": [],
   "source": [
    "pastiche_img = content_img.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 987636,
     "status": "ok",
     "timestamp": 1525909181892,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "CJFrUCyjLoDI",
    "outputId": "ed369170-4391-4eea-ddfd-8dbff065f97a"
   },
   "outputs": [],
   "source": [
    "main(style_img, content_img, pastiche_img,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1003,
     "status": "error",
     "timestamp": 1525939661062,
     "user": {
      "displayName": "Pythontz Pythontz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118343729935310095078"
     },
     "user_tz": -120
    },
    "id": "Q7Fp06SRc3g5",
    "outputId": "85a16edc-6fdb-4e57-86d4-9c03e68e8af4"
   },
   "outputs": [],
   "source": [
    "img = Image.open(\"pastiche_26.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Parrot_ai_Neural_Style_Transfer.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
